{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4I76Kyq--oW"
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertModel, BertTokenizer\n"
      ],
      "metadata": {
        "id": "-VWRploGgsk6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "pyJd1I4bZOeA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, model_name='bert-base-uncased', num_labels_task_a=3, num_labels_task_b=3):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained(model_name)  # for Task 1 / sentence encoding\n",
        "        self.dropout = nn.Dropout(0.1)  # for Task 2\n",
        "        self.task_a = nn.Linear(self.bert.config.hidden_size, num_labels_task_a)  # for Task 2\n",
        "        self.task_b = nn.Linear(self.bert.config.hidden_size, num_labels_task_b)  # for Task 2\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, task='a'):  # for Task 2\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        output = self.dropout(outputs.pooler_output)\n",
        "        if task == 'a':\n",
        "            return self.task_a(output)\n",
        "        elif task == 'b':\n",
        "            return self.task_b(output)"
      ],
      "metadata": {
        "id": "L5TotrvngtgA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1\n"
      ],
      "metadata": {
        "id": "dR4XMibXY9sV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"My favorite animal is the dog.\",\n",
        "            \"I am a Georgia Tech graduate.\",\n",
        "            \"I don't like missing out on savings.\"]\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "model = TransformerModel()\n",
        "\n",
        "with torch.no_grad():\n",
        "  embeddings = model.bert(**inputs).pooler_output"
      ],
      "metadata": {
        "id": "TR9tJo7pFHV4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(embeddings)\n",
        "print(embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFathgazvyD_",
        "outputId": "6c2104bf-d745-482a-b8d6-ef83df32c221"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.8644, -0.3422, -0.8039,  ..., -0.5739, -0.6053,  0.8960],\n",
            "        [-0.8271, -0.5030, -0.8600,  ..., -0.8143, -0.6632,  0.8940],\n",
            "        [-0.7990, -0.4106, -0.8943,  ..., -0.7665, -0.6276,  0.9231]])\n",
            "torch.Size([3, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose to use the pre-trained bert-base-uncased model as my transformer backbone because it is an effective and well-documented option for simpler NLP use cases. To obtain the word embeddings, I first tokenized the example sentences using the bert-base-uncased tokenizer. This splits the text into subwords recognized by the model and assigns them their corresponding ID in the model's vocabulary. Then, I input them into the model and received the embedding as an output. The pooler output is used for sentence-based tasks like sentence classification because it represents whole input sentences rather than individual tokens."
      ],
      "metadata": {
        "id": "_O-dufp0ZZda"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2"
      ],
      "metadata": {
        "id": "VEW3spbRd41t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Task 1, I only needed to use the pre-trained Bert model. For Task 2, I first added a dropout layer; a dropout layer can reduce overfitting, prevent the network from becoming overly reliant on specific nodes, and help increase the usage of other nodes. Finally, I added one linear layer for Task A (sentence classification) and another for Task B (Sentiment Analysis). The labels for Task A (Sentence Classification) would be animals, school, and savings, and the labels for Task B would be positive, neutral, and negative."
      ],
      "metadata": {
        "id": "QvMaABAnjCnc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3"
      ],
      "metadata": {
        "id": "MnwYnm0ycjNt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training: <br>\n",
        "1. If the whole model is frozen, it cannot be trained because none of the model's weights can be updated. Although this means that the model will likely be suboptimal for more specific use cases, it also means that the model can be used immediately without training and will not overfit our current dataset. These benefits make fully frozen models useful for situations where we want to build a quick prototype with limited compute or if we have a limited dataset. This type of model should be used with very limited datasets for quick tasks, such as prototyping a more complex model.\n",
        "2. If the transformer backbone is frozen, this means that the linear layers for tasks A and B can still adapt to the training dataset, but the main BERT model's will remain steady. This setup has similar but less extreme pros and cons compared to freezing the whole model. Because the transformer backbone is frozen, training will not take as much time or computing power. Additionally, it reduces the risk of overfitting to limited datasets. A key downside of this setup is that the transformer backbone's weights will not be able to adapt to the specific tasks the model performs. This type of model should be trained for simple tasks with limited datasets; if the task is complex or we have a large dataset, freezing the backbone could limit the potential of our model.\n",
        "3. If only one task-specific head is frozen (let's say Task B), this will keep Task B performance stable during training while allowing potential for Task A performance to improve. This scenario is therefore best when one of our tasks' performances needs to be improved but we are content with the other's. A possible downside to this approach is that if the transformer backbone's weights shift, Task B's head may not be as compatible with the backbone as it was previously. Thus, when training with this setup, we should ensure that our loss function accounts for Task A and Task B's performance so that the transformer backbone does not neglect Task B performance.\n",
        "\n",
        "Transfer Learning: <br>\n",
        "Transfer learning can be useful in scenarios like our current one, where we apply an existing pre-trained model like bert-base-uncased to new tasks like sentence classification and sentiment analysis. An example of another scenario could be animal classification based on images. We can use a pre-trained model like ResNet50 and fine-tune it with a dataset of animal images. ResNet50 is a good choice because it is trained on the vast ImageNet dataset and can recognize different objects and colors. Of the model's 50 layers, I would freeze the first 10 throughout all of training; these layers primarily detect basic features common to all images such as edges and colors. I would keep the last 5 layers and the classification head unfrozen through the whole training process because these layers primarily focus on more specific features of images. If we are only working with animals and want to focus on differentiating them, it would be beneficial for our model's last few layers to learn the nuances between different images of animals. Finally, I would initially freeze the middle layers and experiment with unfreezing some of them after a few rounds of training; these layers can contain some knowledge that applies to all images and some that may apply to images specifically in the dataset. Thus, unfreezing them could help the model adapt to the task of animal classification, but it may also lead to overfitting our dataset."
      ],
      "metadata": {
        "id": "aERWTy52miUP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4"
      ],
      "metadata": {
        "id": "MAA9rneacoPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PlaceholderDataset(Dataset):\n",
        "    def __len__(self):\n",
        "        return 0\n",
        "\n",
        "    def __getitem__(self, idx):  # should return (input_ids, attention_mask, labels)\n",
        "        raise IndexError(\"This dataset is empty.\")\n",
        "\n",
        "def compute_accuracy(logits, labels):\n",
        "    preds = torch.argmax(logits, dim=1)\n",
        "    return (preds == labels).float().mean().item()"
      ],
      "metadata": {
        "id": "AGSb00PxctbR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader_a = DataLoader(PlaceholderDataset(), batch_size=1)\n",
        "loader_b = DataLoader(PlaceholderDataset(), batch_size=1)\n",
        "\n",
        "model = TransformerModel()\n",
        "combined_batches = list(zip(loader_a, loader_b))\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-2)\n",
        "epochs = 1"
      ],
      "metadata": {
        "id": "neI1IdNcgpmm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  total_acc_a = 0\n",
        "  total_acc_b = 0\n",
        "  for batch_a, batch_b in combined_batches:\n",
        "      input_ids_a, attn_mask_a, labels_a = batch_a\n",
        "      input_ids_b, attn_mask_b, labels_b = batch_b\n",
        "\n",
        "      logits_a = model(input_ids_a, attn_mask_a, task='a')\n",
        "      logits_b = model(input_ids_b, attn_mask_b, task='b')\n",
        "      loss_a = loss_fn(logits_a, labels_a)\n",
        "      loss_b = loss_fn(logits_b, labels_b)\n",
        "      loss = loss_a.item() + loss_b.item()\n",
        "      acc_a = compute_accuracy(logits_a, labels_a)\n",
        "      acc_b = compute_accuracy(logits_b, labels_b)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      total_loss += loss\n",
        "      total_acc_a += acc_a\n",
        "      total_acc_b += acc_b\n",
        "\n",
        "  print(f\"Epoch {epoch}:\")\n",
        "  if (len(loader_a) == 0) or (len(loader_b) == 0):\n",
        "    print(f\"Not enough data\")\n",
        "  else:\n",
        "    print(f\"Loss: {total_loss}, Task A Accuracy: {total_acc_a / len(loader_a)}, \"\n",
        "            f\"Task B Accuracy: {total_acc_b / len(loader_b)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kGrBoFdj0HF",
        "outputId": "ff0554d8-052a-4510-d854-0b93a239fcad"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0:\n",
            "Not enough data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose to use cross entropy loss and the AdamW optimizer because they are commonly used and effective algorithms for optimizing Bert-based models. During training, I handle the MTL framework by alternatively giving the model a Task A sample and a Task B sample rather than sequentially giving it all Task A samples and then all Task B samples. The sequential approach could lead to the model overfitting for Task B if layers of the shared model are not frozen. The forward pass for each task simply involves inputting the input ids and attention mask for each batch of sentences. The attention mask differentiates between real tokens (representing words or subwords) and padding tokens in each sentence. I also chose to represent loss as a combination of the losses for task A and task B for increased efficiency. compute_accuracy() computes the fraction of correct predictions within each batch for each task."
      ],
      "metadata": {
        "id": "ugT8rwVo9Ods"
      }
    }
  ]
}